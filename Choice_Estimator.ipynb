{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#A code to implement the choice model discussed w/ a student\n",
    "#Jeremy Kedziora\n",
    "#24 Apr 2016\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "#import libraries\n",
    "import numpy as np    #import for arrays\n",
    "from scipy.optimize import minimize    #import for optimization\n",
    "import scipy.stats    #import for distributions\n",
    "from scipy.stats import t    #import for hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define functions\n",
    "def maker(N,n_vars):\n",
    "    \"\"\"A function to generate Monte Carlo linear regression data\"\"\"\n",
    "    x = []    #an empty list to hold the data\n",
    "    y = np.zeros(N)    #an array to hold the dependent variable\n",
    "    b = []    #an empty list to hold the true bs\n",
    "    mu = [0.0]    #an empty list to hold the true cutoffs in the ordered probit\n",
    "    i = 1\n",
    "    while i <= n_vars:    #loop over the variables we want to create\n",
    "        x_i = np.random.normal(loc = 0.0, scale = 1.0, size = N)    #generate the data\n",
    "        x.append(x_i)    #add it to the list of data\n",
    "        b_i = np.random.normal(loc = 0.0, scale = 1.0)    #draw a random effect for this variable\n",
    "        b.append(b_i)    #add it to the list of effects\n",
    "        y = y + b_i*x_i    #add the variable effect to the dependent variable\n",
    "        i += 1    #index up i\n",
    "    \n",
    "    x.append(np.ones(N))    #and a column of ones for a constant\n",
    "    b_i = np.random.uniform(0.0,1.0)    #draw a random intercept\n",
    "    \n",
    "    V = [-1,1,1.5]#sorted(np.random.uniform(0,1.5,3))    #create the value for each outcome\n",
    "    y = b_i + y + np.random.normal(loc = 0.0, scale = 1.0, size = N)    #add the normally distributed error term and the intercept\n",
    "    mu = [-float('inf'),0,np.random.uniform(1,1.5,1)[0],float('inf')]    #append the cutoff for the next category\n",
    "    y_cat = (y<0)*0    #set the ys less than 0 to fail\n",
    "    y_cat = y_cat + (0<y)*(y<mu[2])*1    #code all the ys that fall between them to pass\n",
    "    y_cat = y_cat + (y>mu[2])*2    #and code all the ys larger than the largest to distinction\n",
    "\n",
    "    p_fail = scipy.stats.norm.cdf(- y)    #compute the probability of failure\n",
    "    p_pass = scipy.stats.norm.cdf(mu[2] - y)    #compute the probability of passing\n",
    "    p_distinction = 1 - scipy.stats.norm.cdf(mu[2] - y)    #compute the probability of distinction\n",
    "    p_NW = scipy.stats.norm.cdf(p_fail*V[0] + p_pass*V[1] + p_distinction*V[2])    #compute the probability of not withdrawing\n",
    "    p_W = 1 - p_NW    #compute the probability of withdrawing\n",
    "\n",
    "    y_choice = []    #a list to hold the w/nw choice\n",
    "    for i in p_NW:    #loop over obserations\n",
    "        y_choice.append(np.random.binomial(1,i,1)[0])    #choose a w/nw\n",
    "    y_choice = np.array(y_choice)    #make an array\n",
    "    y = (y_choice==0)*1 + (y_choice==1)*(y_cat==0)*2 + (y_choice==1)*(y_cat==1)*3 + (y_choice==1)*(y_cat==2)*4    #and code\n",
    "\n",
    "    b.append(b_i)    #append the constant\n",
    "    \n",
    "    return [np.array(x).T,np.array(y),np.array(b),np.array(mu),V]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choice_mle(b,X,y,V):\n",
    "    \"\"\"A function to compute a choice model with an ordered probit outcome.\n",
    "    Take in:\n",
    "    b: the vector of parameters, the last of which is the cutoff\n",
    "    X: a numpy array of features\n",
    "    y: a numpy array of outcomes where y = 1 is W, y = 2 is NW,F, y = 3 is NW,P, y = 4 is NW,D\n",
    "    V: a list [Value of failing, Value of passing, Value of Disctin]\"\"\"\n",
    "    \n",
    "    xb = X.dot(b[0:(X.shape[1])])    #compute xb\n",
    "    mu = [float('-inf'),0]    #initialize the list of mus\n",
    "    mu = mu + [np.exp(b[len(b) - 1])]    #add the cutoff point to be estimated\n",
    "    mu = mu + [float('inf')]    #append infinity on the end\n",
    "    #print(mu)\n",
    "    p_fail = scipy.stats.norm.cdf(- xb)    #compute the probability of failure\n",
    "    p_pass = scipy.stats.norm.cdf(mu[2] - xb)    #compute the probability of passing\n",
    "    p_distinction = 1 - scipy.stats.norm.cdf(mu[2] - xb)    #compute the probability of distinction\n",
    "    p_NW = scipy.stats.norm.cdf(p_fail*V[0] + p_pass*V[1] + p_distinction*V[2])    #compute the probability of not withdrawing\n",
    "    p_W = 1 - p_NW    #compute the probability of withdrawing\n",
    "    \n",
    "    #assume y = 1 is W, y = 2 is NW,F, y = 3 is NW,P, y = 4 is NW,D\n",
    "    log_probs = ((y==1)*np.log(p_W) + (y==2)*(np.log(p_NW) + np.log(p_fail)) \n",
    "                 + (y==3)*(np.log(p_NW) + np.log(p_pass)) + (y==4)*(np.log(p_NW) + np.log(p_distinction)))\n",
    "    \n",
    "    return -1*sum(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "n_vars = 3\n",
    "Data = maker(N,n_vars)\n",
    "X = Data[0]\n",
    "y = Data[1]\n",
    "\n",
    "b = np.array(list(np.random.uniform(0,1,X.shape[1])*0.01) + list(np.random.uniform(0,1.0,1)))    #set starting values\n",
    "\n",
    "choice_mle(b,X,y,V=[-1,1,1.5])\n",
    "\n",
    "V_F = np.linspace(0,2,3)\n",
    "V_P = np.linspace(-2,0,3)\n",
    "V_D = np.linspace(2,3,3)\n",
    "\n",
    "V = [[x,y,z] for x in V_F for y in V_P for z in V_D]\n",
    "\n",
    "models = []\n",
    "for v in V:\n",
    "    print(v)\n",
    "    model = minimize(choice_mle, x0 = b, args = (X,y,v), method = 'Nelder-Mead',options={'maxiter':2000})    #maximize the log-likelihood\n",
    "    models.append(model)\n",
    "#print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
