{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#A code to run ordinary least squares with associated statistics\n",
    "#Jeremy Kedziora\n",
    "#24 March 2016\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "#import libraries\n",
    "import numpy as np    #import for arrays\n",
    "from scipy.optimize import minimize    #import for optimization\n",
    "import scipy.stats\n",
    "import statsmodels as sm\n",
    "from scipy.stats import t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define functions\n",
    "def maker(N,n_vars,kind = 'linear',n_cat = 0):\n",
    "    \"\"\"A function to generate Monte Carlo linear regression data\"\"\"\n",
    "    x = []    #an empty list to hold the data\n",
    "    y = np.zeros(N)    #an array to hold the dependent variable\n",
    "    b = []    #an empty list to hold the true bs\n",
    "    mu = [0.0]    #an empty list to hold the true cutoffs in the ordered probit\n",
    "    i = 1\n",
    "    while i <= n_vars:    #loop over the variables we want to create\n",
    "        x_i = np.random.normal(loc = 0.0, scale = 1.0, size = N)    #generate the data\n",
    "        x.append(x_i)    #add it to the list of data\n",
    "        b_i = np.random.normal(loc = 0.0, scale = 1.0)    #draw a random effect for this variable\n",
    "        b.append(b_i)    #add it to the list of effects\n",
    "        y = y + b_i*x_i    #add the variable effect to the dependent variable\n",
    "        i += 1    #index up i\n",
    "    \n",
    "    x.append(np.ones(N))    #and a column of ones for a constant\n",
    "    b_i = np.random.uniform(0.0,1.0)    #draw a random intercept\n",
    "\n",
    "    if kind == 'linear':\n",
    "        y = b_i + y + np.random.normal(loc = 0.0, scale = 1.0, size = N)    #add the normally distributed error term and the intercept\n",
    "    if kind == 'logit':\n",
    "        y = (np.random.uniform(0,1,len(y)) < np.exp(b_i + y)/(1 + np.exp(b_i + y)))*1    #draw y values\n",
    "    if kind == 'ordered':\n",
    "        y = b_i + y + np.random.normal(loc = 0.0, scale = 1.0, size = N)    #add the normally distributed error term and the intercept\n",
    "        for i in range(n_cat-2):    #loop over number of categories\n",
    "            mu.append(mu[i] + np.random.uniform(0.25,1,1)[0])    #append the cutoff for the next category\n",
    "        y_cat = (y<mu[0])*0    #set the ys less than 0 to category 0\n",
    "        for i in range(1,len(mu)):    #loop over the remaining categories\n",
    "            y_cat = y_cat + (mu[i-1]<y)*(y<mu[i])*i    #code all the ys that fall between them\n",
    "        y_cat = y_cat + (y>mu[len(mu)-1])*(n_cat - 1)    #and code all the ys larger than the largest\n",
    "        y = y_cat    #and save\n",
    "    if kind == 'count':\n",
    "        b_i = np.random.uniform(-1.0,0)    #draw a random intercept\n",
    "        y = np.random.poisson(np.exp(b_i + y))    #draw the counts\n",
    "    if kind == 'duration':\n",
    "        alpha = np.random.uniform(0,2)    #draw the shape parameter\n",
    "        lambda_xb = np.exp(b_i + y)    #make the lambda parameter which varies by observation\n",
    "        y = lambda_xb*(-1*np.log(np.random.uniform(0,1,N)))**(1/alpha)    #draw durations from the weibull\n",
    "    b.append(b_i)    #append this intercept to the effects\n",
    "    if kind == 'duration':\n",
    "        b.append(alpha)\n",
    "    return [np.array(x).T,np.array(y),np.array(b),np.array(mu)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logit_mle(b,X,y):\n",
    "    \"\"\"A function to compute logit coefficients using MLE\"\"\"\n",
    "    xb = X.dot(b)    #compute the means\n",
    "    return -1*sum(y*xb - np.log(1+np.exp(xb)))    #return the log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logit_effects(b,X,which_variable):\n",
    "    \"\"\"A function to compute the predicted probability as x varies\"\"\"\n",
    "    x_mean = np.mean(X,0)    #compute the means\n",
    "    grid = np.linspace(min(X[:,which_variable]),max(X[:,which_variable]),100)    #make a grid\n",
    "    probs = []    #a list to hold the predicted probabilities\n",
    "    for g in grid:    #loop over grid\n",
    "        x_mean[which_variable] = g    #replace\n",
    "        probs = probs + [np.exp(x_mean.dot(b))/(1 + np.exp(x_mean.dot(b)))]    #compute the logit transform\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poisson_mle(b,X,y):\n",
    "    \"\"\"A function to compute the poisson log-likelihood.\"\"\"\n",
    "    xb = X.dot(b)    #compute xb\n",
    "    return -1*sum(y*xb - np.exp(xb))    #compute the log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poisson_effects(b,X,which_variable):\n",
    "    \"\"\"A function to compute the predicted probability as x varies\"\"\"\n",
    "    x_mean = np.mean(X,0)    #compute the means\n",
    "    grid = np.linspace(min(X[:,which_variable]),max(X[:,which_variable]),100)    #make a grid\n",
    "    lambdas = []    #a list to hold the predicted probabilities\n",
    "    for g in grid:    #loop over grid\n",
    "        x_mean[which_variable] = g    #replace\n",
    "        lambdas = lambdas + [np.exp(x_mean.dot(b))]    #compute the logit transform\n",
    "    return lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ordered_probit_mle(b,X,y):\n",
    "    \"\"\"A function to compute the ordered probit log-likelihood.\"\"\"\n",
    "    xb = X.dot(b[0:(X.shape[1])])    #compute xb\n",
    "    mu = [float('-inf'),0]    #initialize the list of mus\n",
    "    for i in range(len(b[X.shape[1]:])):    #loop over categories\n",
    "        mu = mu + [mu[i+1] + b[X.shape[1]:][i]]    #and create each mu\n",
    "    mu = mu + [float('inf')]    #append infinity on the end\n",
    "    probs = np.zeros(len(y))    #set up an array of 0s\n",
    "    for i in range(1,len(set(y)) + 1):    #loop over categories\n",
    "        probs = probs + (scipy.stats.norm.cdf(mu[i] - xb) - scipy.stats.norm.cdf(mu[i - 1] - xb))*(y == list(set(y))[i-1])    #compute probability\n",
    "    return -1*sum(np.log(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OP_predicted_values(b,X):\n",
    "    \"\"\"A function to compute predicted values in the ordered probit.\"\"\"\n",
    "    xb = X.dot(b[0:(X.shape[1])])    #compute xb\n",
    "    mu = [float('-inf'),0]    #initialize the list of mus\n",
    "    for i in range(len(b[X.shape[1]:])):    #loop over categories\n",
    "        mu = mu + [mu[i+1] + b[X.shape[1]:][i]]    #and create each mu\n",
    "    mu = mu + [float('inf')]    #append infinity on the end\n",
    "    y_pred = np.zeros(len(xb))    #set up an array of zeros\n",
    "    for i in range(1,len(mu)):    #loop over categories\n",
    "        y_pred = y_pred + (((mu[i-1]<xb)*1*(mu[i]>xb)*1)*(i-1))    #check which category each falls in\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weibull_mle(b,X,t):\n",
    "    \"\"\"A function to compute the log-log mle.\"\"\"\n",
    "    alpha = np.exp(b[len(b) - 1])    #grab the shape parameter\n",
    "    xb = X.dot(b[0:(X.shape[1])])    #grab the covariate effects parameter\n",
    "    return -1*sum(np.log(alpha) + alpha*xb + (alpha - 1)*(np.log(t)) - (t*np.exp(xb))**alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mle_inferences(model):\n",
    "    \"\"\"A function to compute inferences from an MLE model.\"\"\"\n",
    "    Coefficients = model.x    #pull out the coefficients\n",
    "    SE = np.diag(model.hess_inv)**0.5\n",
    "    t_stat = Coefficients/SE    #compute the t statistics for each variable\n",
    "    p_values = 2*t.pdf(abs(t_stat),df = N - 1)    #compute the p-values\n",
    "    print('Coefficients are:         ',np.round(Coefficients,4))    #print coefficients rounded to 4th decimal\n",
    "    print('Standard Errors are:      ',np.round(SE,4))    #print standard errors round to 4th decimal\n",
    "    print('t-statistics are:         ',np.round(t_stat,4))    #print tstat rounded to 4th decimal\n",
    "    print('p-values are:             ',np.round(p_values,4))    #print p-value rounded to 4th decimal\n",
    "    CI = []    #an empty list to hold the CI strings\n",
    "    for i,j in zip(Coefficients,SE):    #loop over variables\n",
    "        CI.append('[' + str(round(i-1.96*j,4)) + ',' + str(round(i+1.96*j,4)) + ']')    #create the CI\n",
    "    print('Confidence Intervals are: ',CI)    #print the 95% CI rounded to 4th decimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "n_vars = 3\n",
    "Data = maker(N,n_vars,kind = 'logit')    #make logit data\n",
    "X = Data[0]    #pull out explanatory variables\n",
    "y = Data[1]    #pull out dependent variable\n",
    "b = Data[2]    #pull out true coefficients\n",
    "\n",
    "b = np.random.uniform(0,1,4)*0.01    #set starting values\n",
    "model = minimize(logit_mle, x0 = b, args = (X,y), method = 'BFGS')    #maximize the log-likelihood\n",
    "print(mle_inferences(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logit_effects(coefficients,X,0)    #compute effect on the predicted probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "n_vars = 3\n",
    "Data = maker(N,n_vars,'count')    #make poisson data\n",
    "X = Data[0]    #pull out the features\n",
    "y = Data[1]    #pull out the labels\n",
    "\n",
    "b = np.random.uniform(0,1,4)*0.01    #set starting values\n",
    "model = minimize(poisson_mle, x0 = b, args = (X,y), method = 'Nelder-Mead')    #maximize the log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poisson_effects(coefficients,X,0)    #compute the effect on the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 10000\n",
    "n_vars = 3\n",
    "Data = maker(N,n_vars,'ordered',n_cat=3)    #make poisson data\n",
    "X = Data[0]    #pull out the features\n",
    "y = Data[1]    #pull out the labels\n",
    "\n",
    "b = np.array(list(np.random.uniform(0,1,X.shape[1])*0.01) + list(np.random.uniform(0,1.0,len(set(y))-2)))    #set starting values\n",
    "coefficients = minimize(ordered_probit_mle, x0 = b, args = (X,y), method = 'Nelder-Mead').x    #maximize the log-likelihood\n",
    "\n",
    "print('Percent correctly predicted is: ',sum((OP_predicted_values(coefficients,X) - y)==0)/N)    #compute the PCP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "n_vars = 3\n",
    "Data = maker(N,n_vars,'duration')    #make poisson data\n",
    "X = Data[0]    #pull out the features\n",
    "t = Data[1]    #pull out the labels\n",
    "\n",
    "b = np.array(np.random.uniform(0,1,X.shape[1]+1)*0.01)    #set starting values\n",
    "coefficients = minimize(weibull_mle, x0 = b, args = (X,t), method = 'BFGS').x    #maximize the log-likelihood\n",
    "print(coefficients[:len(coefficients)-1],np.exp(coefficients[len(coefficients)-1]))\n",
    "print(Data[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
